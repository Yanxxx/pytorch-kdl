{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import cv2 # for resize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406] \n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './datasets/key_frame_identifier/block-insertion-test/'\n",
    "img_folder = data_path + 'color/'\n",
    "depth_folder = data_path + 'depth/'\n",
    "pcl_file = '000000-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 160, 3])\n",
      "torch.Size([120, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True, FigureClass=<class 'matplotlib.figure.Figure'>, clear=False, **kwargs)>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(img_folder + pcl_file + '.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "img = data[0,0,:,:,:]\n",
    "img = cv2.resize(img, dsize=(160, 120), interpolation=cv2.INTER_CUBIC)\n",
    "plt.imshow(img)\n",
    "img = img / 255.0\n",
    "img = img - mean\n",
    "img = img / std\n",
    "with open(depth_folder + pcl_file + '.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "depth = data[0,0,:,:]\n",
    "sd = depth\n",
    "depth = cv2.resize(depth, dsize=(160, 120), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "color = torch.Tensor(img)\n",
    "print(color.size())\n",
    "depth = torch.Tensor(depth)\n",
    "print(depth.size())\n",
    "\n",
    "plt.imshow(depth)\n",
    "plt.figure\n",
    "# plt.imshow(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 160, 1])\n"
     ]
    }
   ],
   "source": [
    "depth = torch.reshape(depth, (120,160,1))\n",
    "print(depth.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 160, 4])\n"
     ]
    }
   ],
   "source": [
    "data = torch.cat((color, depth), 2)\n",
    "print(data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 120, 160])\n"
     ]
    }
   ],
   "source": [
    "data = data.permute(2, 0 ,1)\n",
    "data = torch.reshape(data, (1, 4, 120, 160))\n",
    "print(data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 21, 31])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(4, 32, 3),\n",
    "    nn.Conv2d(32, 32, 3),\n",
    "    nn.Conv2d(32, 32, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, stride=2),\n",
    "    nn.Conv2d(64, 128, 3),\n",
    "    nn.Conv2d(128, 128, 3),\n",
    "    nn.ReLU(),    \n",
    "    nn.Conv2d(128, 256, 3, stride=2),\n",
    "    nn.Conv2d(256, 256, 3),\n",
    "    nn.Conv2d(256, 256, 3),\n",
    "    nn.ReLU(),    \n",
    "    nn.Conv2d(256, 196, 1)    \n",
    ")\n",
    "\n",
    "output = model(data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SpatialSoftmax(torch.nn.Module):\n",
    "    def __init__(self, height, width, channel, temperature=None, data_format='NCHW'):\n",
    "        super(SpatialSoftmax, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channel = channel\n",
    "\n",
    "        if temperature:\n",
    "            self.temperature = Parameter(torch.ones(1)*temperature)\n",
    "        else:\n",
    "            self.temperature = 1.\n",
    "\n",
    "        pos_x, pos_y = np.meshgrid(\n",
    "                np.linspace(-1., 1., self.height),\n",
    "                np.linspace(-1., 1., self.width)\n",
    "                )\n",
    "        pos_x = torch.from_numpy(pos_x.reshape(self.height*self.width)).float()\n",
    "        pos_y = torch.from_numpy(pos_y.reshape(self.height*self.width)).float()\n",
    "        self.register_buffer('pos_x', pos_x)\n",
    "        self.register_buffer('pos_y', pos_y)\n",
    "\n",
    "    def forward(self, feature, depth):\n",
    "        # Output:\n",
    "        #   (N, C*2) x_0 y_0 ...\n",
    "        if self.data_format == 'NHWC':\n",
    "            feature = feature.transpose(1, 3).tranpose(2, 3).view(-1, self.height*self.width)\n",
    "        else:\n",
    "            feature = feature.view(-1, self.height*self.width)\n",
    "\n",
    "        softmax_attention = F.softmax(feature/self.temperature, dim=-1)\n",
    "        expected_x = torch.sum(self.pos_x*softmax_attention, dim=1, keepdim=True)\n",
    "        expected_y = torch.sum(self.pos_y*softmax_attention, dim=1, keepdim=True)\n",
    "#         expected_xy = torch.cat([expected_x, expected_y], 1)\n",
    "#         feature_keypoints = expected_xy.view(-1, self.channel*2)\n",
    "        \n",
    "        image_height = depth.shape[3]\n",
    "        image_weight = depth.shape[2]\n",
    "        \n",
    "        ix = torch.round(expected_x * image_weight).long()\n",
    "        iy = torch.round(expected_y * image_height).long()\n",
    "        \n",
    "        z = depth[:, 0, ix, iy]\n",
    "        \n",
    "        result = torch.cat([expected_x * z, expected_y * z, z], 1)\n",
    "        feature_keypoints = result.view(-1, self.channel*3)\n",
    "\n",
    "        return feature_keypoints\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_softargmax = SpatialSoftmax(output.shape[3], output.shape[2], output.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = torch.reshape(torch.Tensor(sd), [1,1,480,640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 588])\n"
     ]
    }
   ],
   "source": [
    "pts = spatial_softargmax(output, depth)\n",
    "print(pts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588\n"
     ]
    }
   ],
   "source": [
    "print(pts.shape[1])\n",
    "length = int(pts.shape[1] / 3)\n",
    "rppts = torch.reshape(pts, [length, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler2rotm(theta):\n",
    "    R_x = np.array([[1,         0,                  0                   ],\n",
    "                    [0,         math.cos(theta[0]), -math.sin(theta[0]) ],\n",
    "                    [0,         math.sin(theta[0]), math.cos(theta[0])  ]\n",
    "                    ])\n",
    "    R_y = np.array([[math.cos(theta[1]),    0,      math.sin(theta[1])  ],\n",
    "                    [0,                     1,      0                   ],\n",
    "                    [-math.sin(theta[1]),   0,      math.cos(theta[1])  ]\n",
    "                    ])         \n",
    "    R_z = np.array([[math.cos(theta[2]),    -math.sin(theta[2]),    0],\n",
    "                    [math.sin(theta[2]),    math.cos(theta[2]),     0],\n",
    "                    [0,                     0,                      1]\n",
    "                    ])            \n",
    "    R = np.dot(R_z, np.dot( R_y, R_x ))\n",
    "    return R\n",
    "\n",
    "def point_in_camera(point, rotation, translation):\n",
    "    reprojected_pt = torch.matmul(rotation,point) + translation#[:,None]\n",
    "    return reprojected_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = euler2rotm([np.pi / 4, np.pi, -np.pi / 2])\n",
    "position = np.array([1.0, 0, 0.75]).reshape((3,1))\n",
    "\n",
    "trans = np.concatenate([np.concatenate([rotation, position], axis=1), np.array([0,0,0,1]).reshape(1,4)])\n",
    "irotation = trans[0:3, 0:3]\n",
    "itranslation = trans[0:3,3:]\n",
    "trans = torch.Tensor(np.linalg.inv(trans))\n",
    "frotation = trans[0:3, 0:3]\n",
    "ftranslation = trans[0:3,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = point_in_camera(torch.transpose(rppts, 1, 0).to(device), frotation.to(device), ftranslation.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196])\n"
     ]
    }
   ],
   "source": [
    "print(pp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation(nn.Module):\n",
    "    \n",
    "    def __init__(self, rotation, translation):\n",
    "        super().__init__()\n",
    "        self.rotation = rotation\n",
    "        self.translation = translation\n",
    "        self.br = torch.transpose(rotation, 1, 0)\n",
    "        self.bt = torch.matmul(self.br, translation)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        print(data.shape)\n",
    "        dim = int(data.shape[1] / 3)\n",
    "        pts = torch.transpose(data.reshape(data.shape[0], dim,  3), 1, 0)\n",
    "        reprojected_pt = torch.matmul(self.rotation,pts) + self.translation#[:,None]\n",
    "        return reprojected_pt\n",
    "    \n",
    "    def backward(self, data):\n",
    "        dim = int(data.shape[1] / 3)\n",
    "        pts = torch.transpose(data.reshape(data.shape[0], dim, 3), 1, 0)\n",
    "        reprojected_pt = torch.matmul(self.br,pts) + self.bt#[:,None]\n",
    "        return reprojected_pt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 196])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[65, 3]' is invalid for input of size 588",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-0eb59d83fd81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtransformlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-251-0e2d8fbb9e80>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mreprojected_pt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;31m#[:,None]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreprojected_pt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[65, 3]' is invalid for input of size 588"
     ]
    }
   ],
   "source": [
    "rotation = torch.Tensor(frotation).to(device)\n",
    "translation = torch.Tensor(ftranslation).to(device)\n",
    "transformlayer = Transformation(rotation, translation)\n",
    "transformlayer.to(device)\n",
    "\n",
    "output = transformlayer(pp.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([588])\n"
     ]
    }
   ],
   "source": [
    "attention = torch.flatten(output)\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = nn.Sequential(\n",
    "    nn.Linear(588, 294),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(294, 147),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(147, 42),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(42, 21),\n",
    "    nn.Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.to(device)\n",
    "coord = regression(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "print(coord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureNet(nn.Sequential):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(4, 32, 3),\n",
    "          nn.Conv2d(32, 32, 3),\n",
    "          nn.Conv2d(32, 32, 3),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(32, 64, 3, stride=2),\n",
    "          nn.Conv2d(64, 128, 3),\n",
    "          nn.Conv2d(128, 128, 3),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(),    \n",
    "          nn.Conv2d(128, 256, 3, stride=2),\n",
    "          nn.Conv2d(256, 256, 3),\n",
    "          nn.Conv2d(256, 256, 3),\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.ReLU(),    \n",
    "          nn.Conv2d(256, 196, 1),\n",
    "          nn.BatchNorm2d(196),\n",
    "          nn.ReLU(),     \n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FeatureNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = f(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 21, 31])\n"
     ]
    }
   ],
   "source": [
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "\n",
    "class ExtendedSpatialSoftargMax(nn.Module):\n",
    "  \n",
    "    def __init__(self, height, width, channel, temperature=None, data_format='NCHW'):\n",
    "        super(ExtendedSpatialSoftargMax, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channel = channel\n",
    "\n",
    "        if temperature:\n",
    "            self.temperature = Parameter(torch.ones(1)*temperature)\n",
    "        else:\n",
    "            self.temperature = 1.\n",
    "\n",
    "        pos_x, pos_y = np.meshgrid(\n",
    "                np.linspace(-1., 1., self.height),\n",
    "                np.linspace(-1., 1., self.width)\n",
    "                )\n",
    "        pos_x = torch.from_numpy(pos_x.reshape(self.height*self.width)).float()\n",
    "        pos_y = torch.from_numpy(pos_y.reshape(self.height*self.width)).float()\n",
    "        self.register_buffer('pos_x', pos_x)\n",
    "        self.register_buffer('pos_y', pos_y)\n",
    "\n",
    "    def forward(self, feature, depth):\n",
    "        # Output:\n",
    "        #   (N, C*2) x_0 y_0 ...\n",
    "        if self.data_format == 'NHWC':\n",
    "            feature = feature.transpose(1, 3).tranpose(2, 3).view(-1, self.height*self.width)\n",
    "        else:\n",
    "            feature = feature.view(-1, self.height*self.width)\n",
    "\n",
    "        softmax_attention = F.softmax(feature/self.temperature, dim=-1)\n",
    "        expected_x = torch.sum(self.pos_x*softmax_attention, dim=1, keepdim=True)\n",
    "        expected_y = torch.sum(self.pos_y*softmax_attention, dim=1, keepdim=True)\n",
    "#         expected_xy = torch.cat([expected_x, expected_y], 1)\n",
    "#         feature_keypoints = expected_xy.view(-1, self.channel*2)\n",
    "        \n",
    "        image_height = depth.shape[3]\n",
    "        image_weight = depth.shape[2]\n",
    "        \n",
    "        ix = torch.round(expected_x * image_weight).long()\n",
    "        iy = torch.round(expected_y * image_height).long()\n",
    "        \n",
    "        z = depth[:, 0, ix, iy] \n",
    "        z_prime_x = z / 900 * image_weight\n",
    "        z_prime_y = z / 900 * image_height\n",
    "        \n",
    "        result = torch.cat([expected_x * z_prime_x, expected_y * z_prime_y, z], 1)\n",
    "        feature_keypoints = result.view(-1, self.channel*3)\n",
    "\n",
    "        return feature_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess = ExtendedSpatialSoftargMax(k.shape[3], k.shape[2], k.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 21, 31])\n"
     ]
    }
   ],
   "source": [
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou = ess(k, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseRegression(nn.Sequential):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Linear(588, 294),\n",
    "          nn.BatchNorm1d(294),\n",
    "          nn.Linear(294, 147),\n",
    "          nn.BatchNorm1d(147),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(147, 42),\n",
    "          nn.BatchNorm1d(42),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(42, 21),\n",
    "          nn.BatchNorm1d(21),\n",
    "          nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.model(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyframe(nn.Module):\n",
    "    def __init__(self, rotation, translation):\n",
    "        super(Keyframe, self).__init__()\n",
    "        self.feature = FeatureNet()\n",
    "        self.extended_spatial_max = ExtendedSpatialSoftargMax(31,21,196)\n",
    "        self.transform = Transformation(rotation, translation)\n",
    "        self.pose_regress = PoseRegression()\n",
    "        self.rotation = rotation\n",
    "        self.translation = translation\n",
    "              \n",
    "    def forward(self, data, depth):\n",
    "        output = self.feature(data)\n",
    "        output = self.extended_spatial_max(output, depth)\n",
    "        output = self.transform(output)\n",
    "        output = self.pose_regress(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyf = Keyframe(rotation, translation).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 588])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-a1d21de80539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-266-dfd34d562728>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, depth)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended_spatial_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_regress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-265-a75f5e05c99d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "out = keyf(data.to(device), depth.to(device))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
