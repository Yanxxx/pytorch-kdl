{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83ca7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "    \n",
    "class FeatureNet(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(4, 32, 3),\n",
    "          nn.Conv2d(32, 32, 3),\n",
    "          nn.Conv2d(32, 32, 3),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(32, 64, 3, stride=2),\n",
    "          nn.Conv2d(64, 128, 3),\n",
    "          nn.Conv2d(128, 128, 3),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(),    \n",
    "          nn.Conv2d(128, 256, 3, stride=2),\n",
    "          nn.Conv2d(256, 256, 3),\n",
    "          nn.Conv2d(256, 256, 3),\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.ReLU(),    \n",
    "          nn.Conv2d(256, 196, 1),\n",
    "          nn.BatchNorm2d(196),\n",
    "          nn.ReLU(),     \n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "class PoseRegression(nn.Sequential):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Linear(588, 294),\n",
    "          nn.BatchNorm1d(294),\n",
    "          nn.Linear(294, 147),\n",
    "          nn.BatchNorm1d(147),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(147, 42),\n",
    "          nn.BatchNorm1d(42),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(42, 21),\n",
    "          nn.BatchNorm1d(21),\n",
    "          nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "    \n",
    "\n",
    "class Transformation(nn.Module):\n",
    "    \n",
    "    def __init__(self, rotation, translation):\n",
    "        super().__init__()\n",
    "        self.rotation = rotation\n",
    "        self.translation = translation\n",
    "        self.br = torch.transpose(rotation, 1, 0)\n",
    "        self.bt = torch.matmul(self.br, translation)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        reprojected_pt = torch.matmul(self.rotation,data) + self.translation#[:,None]\n",
    "        return reprojected_pt\n",
    "    \n",
    "    def backward(self, data):\n",
    "        reprojected_pt = torch.matmul(self.br,data) + self.bt#[:,None]\n",
    "        return reprojected_pt\n",
    "        \n",
    "class ExtendedSpatialSoftargMax(nn.Module):\n",
    "  \n",
    "    def __init__(self, height, width, channel, temperature=None, data_format='NCHW'):\n",
    "        super(ExtendedSpatialSoftargMax, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channel = channel\n",
    "\n",
    "        if temperature:\n",
    "            self.temperature = Parameter(torch.ones(1)*temperature)\n",
    "        else:\n",
    "            self.temperature = 1.\n",
    "\n",
    "        pos_x, pos_y = np.meshgrid(\n",
    "                np.linspace(-1., 1., self.height),\n",
    "                np.linspace(-1., 1., self.width)\n",
    "                )\n",
    "        pos_x = torch.from_numpy(pos_x.reshape(self.height*self.width)).float()\n",
    "        pos_y = torch.from_numpy(pos_y.reshape(self.height*self.width)).float()\n",
    "        self.register_buffer('pos_x', pos_x)\n",
    "        self.register_buffer('pos_y', pos_y)\n",
    "\n",
    "    def forward(self, feature, depth):\n",
    "        # Output:\n",
    "        #   (N, C*2) x_0 y_0 ...\n",
    "        if self.data_format == 'NHWC':\n",
    "            feature = feature.transpose(1, 3).tranpose(2, 3).view(-1, self.height*self.width)\n",
    "        else:\n",
    "            feature = feature.view(-1, self.height*self.width)\n",
    "\n",
    "        softmax_attention = F.softmax(feature/self.temperature, dim=-1)\n",
    "        expected_x = torch.sum(self.pos_x*softmax_attention, dim=1, keepdim=True)\n",
    "        expected_y = torch.sum(self.pos_y*softmax_attention, dim=1, keepdim=True)\n",
    "#         expected_xy = torch.cat([expected_x, expected_y], 1)\n",
    "#         feature_keypoints = expected_xy.view(-1, self.channel*2)\n",
    "        \n",
    "        image_height = depth.shape[3]\n",
    "        image_weight = depth.shape[2]\n",
    "        \n",
    "        ix = torch.round(expected_x * image_weight).long()\n",
    "        iy = torch.round(expected_y * image_height).long()\n",
    "        \n",
    "        z = depth[:, 0, ix, iy]\n",
    "        \n",
    "        result = torch.cat([expected_x * z, expected_y * z, z], 1)\n",
    "        feature_keypoints = result.view(-1, self.channel*3)\n",
    "\n",
    "        return feature_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aed1020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyframe(nn.Module):\n",
    "    def __init__(self, rotation, translation):\n",
    "        super(Keyframe, self).__init__()\n",
    "        self.feature = FeatureNet()\n",
    "        self.extended_spatial_max = ExtendedSpatialSoftargMax(31,21,196)\n",
    "        self.transform = Transformation(rotation, translation)\n",
    "        self.pose_regress = PoseRegression()\n",
    "        self.rotation = rotation\n",
    "        self.translation = translation\n",
    "              \n",
    "    def forward(self, data, depth):\n",
    "        output = self.feature(data)\n",
    "        output = self.extended_spatial_max(output, depth)\n",
    "        output = self.transform(output)\n",
    "        output = self.pose_regress(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78775820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def euler2rotm(theta):\n",
    "    R_x = np.array([[1,         0,                  0                   ],\n",
    "                    [0,         math.cos(theta[0]), -math.sin(theta[0]) ],\n",
    "                    [0,         math.sin(theta[0]), math.cos(theta[0])  ]\n",
    "                    ])\n",
    "    R_y = np.array([[math.cos(theta[1]),    0,      math.sin(theta[1])  ],\n",
    "                    [0,                     1,      0                   ],\n",
    "                    [-math.sin(theta[1]),   0,      math.cos(theta[1])  ]\n",
    "                    ])         \n",
    "    R_z = np.array([[math.cos(theta[2]),    -math.sin(theta[2]),    0],\n",
    "                    [math.sin(theta[2]),    math.cos(theta[2]),     0],\n",
    "                    [0,                     0,                      1]\n",
    "                    ])            \n",
    "    R = np.dot(R_z, np.dot( R_y, R_x ))\n",
    "    return R\n",
    "\n",
    "def point_in_camera(point, rotation, translation):\n",
    "    reprojected_pt = torch.matmul(rotation,point) + translation#[:,None]\n",
    "    return reprojected_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ffdf846",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = torch.Tensor(euler2rotm([np.pi / 4, np.pi, -np.pi / 2]))\n",
    "translation = torch.Tensor(np.array([1.0, 0, 0.75]).reshape((3,1)))\n",
    "\n",
    "k = Keyframe(rotation, translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31624fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The Yan Li, UTK, Knoxville, TN.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import torch\n",
    "import cv2 # for resize image\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "import random as r\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, target_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.mean = [0.485, 0.456, 0.406] \n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "        self.image_folder = join(data_dir, 'color')\n",
    "        self.depth_folder = join(data_dir, 'depth')\n",
    "        self.info_folder = join(data_dir, 'info')\n",
    "        self.action_folder = join(data_dir, 'action')\n",
    "        self.image_files = listdir(self.image_folder)\n",
    "        self.depth_files = listdir(self.depth_folder)\n",
    "        self.info_files = listdir(self.info_folder)\n",
    "        self.action_files = listdir(self.action_folder)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # image\n",
    "        data = self.loadfile(join(self.image_folder, self.image_files[idx]))\n",
    "        selected_frame = r.randint(1, data.shape[0])\n",
    "        img = data[selected_frame,0]\n",
    "        img = cv2.resize(img, dsize=(160, 120), interpolation=cv2.INTER_CUBIC)\n",
    "        img = img / 255.0\n",
    "        img = img - self.mean\n",
    "        img = img / self.std\n",
    "        \n",
    "        # depth\n",
    "        data = self.loadfile(join(self.depth_folder, self.depth_files[idx]))            \n",
    "        depth = data[selected_frame,0]\n",
    "        td = cv2.resize(depth, dsize=(160, 120), interpolation=cv2.INTER_CUBIC)        \n",
    "        depth = torch.Tensor(data[selected_frame,0])\n",
    "        color = torch.Tensor(img)\n",
    "        td = torch.Tensor(td)\n",
    "        td = torch.reshape(td, (120,160,1))\n",
    "        data = torch.cat((color, td), 2)        \n",
    "        data = data.permute(2, 0 ,1)\n",
    "        data = torch.reshape(data, (1, 4, 120, 160))\n",
    "        \n",
    "        # info\n",
    "        info = self.loadfile(join(self.info_folder, self.info_files[idx]))\n",
    "        #action \n",
    "        action = self.loadfile(join(self.action_folder, self.action_files[idx]))        \n",
    "        gt = self.resolveInfo(info, action, selected_frame)\n",
    "        \n",
    "        return data, depth, gt\n",
    "    \n",
    "    def loadfile(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def resolveInfo(self, info, action, selected_frame):\n",
    "\n",
    "        x = np.concatenate((np.array(info[selected_frame][5][0]), \n",
    "             np.array(info[selected_frame][5][1])),axis=0)\n",
    "        y = np.concatenate((np.array(info[selected_frame][6][0]), \n",
    "             np.array(info[selected_frame][6][1])), axis=0)\n",
    "        z = np.concatenate((np.array(action[selected_frame]['pose'][0]), \n",
    "             np.array(action[selected_frame]['pose'][1])),axis=0)\n",
    "        \n",
    "        gt = np.concatenate((x,y,z),axis=0)\n",
    "        gt = torch.Tensor(gt)        \n",
    "        return gt\n",
    "        \n",
    "#mean = [0.485, 0.456, 0.406] \n",
    "#std = [0.229, 0.224, 0.225]\n",
    "#\n",
    "#data_path = '../datasets/key_frame_identifier/block-insertion-test/'\n",
    "#img_folder = data_path + 'color/'\n",
    "#depth_folder = data_path + 'depth/'\n",
    "#pcl_file = '000000-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73d26a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'datasets/key_frame_identifier/block-insertion-test'\n",
    "data = dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f264c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.2) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-a185b985020e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-a4eda397e133>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.2) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "img, depth, gt = data[999]\n",
    "print(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "379d99ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(7,1)\n",
    "y = torch.Tensor(7,1)\n",
    "z = torch.Tensor(7,1)\n",
    "\n",
    "o = torch.cat((x,y,z),dim=0)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d12f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
