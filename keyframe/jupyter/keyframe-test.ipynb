{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67de867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "    \n",
    "class FeatureNet(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(4, 32, 3),\n",
    "          nn.Conv2d(32, 32, 3),\n",
    "          nn.Conv2d(32, 32, 3),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(32, 64, 3, stride=2),\n",
    "          nn.Conv2d(64, 128, 3),\n",
    "          nn.Conv2d(128, 128, 3),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(),    \n",
    "          nn.Conv2d(128, 256, 3, stride=2),\n",
    "          nn.Conv2d(256, 256, 3),\n",
    "          nn.Conv2d(256, 256, 3),\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.ReLU(),    \n",
    "          nn.Conv2d(256, 196, 1),\n",
    "          nn.BatchNorm2d(196),\n",
    "          nn.ReLU(),     \n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "class PoseRegression(nn.Sequential):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Linear(588, 294),\n",
    "          nn.BatchNorm1d(294),\n",
    "          nn.Linear(294, 147),\n",
    "          nn.BatchNorm1d(147),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(147, 42),\n",
    "          nn.BatchNorm1d(42),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(42, 21),\n",
    "          nn.BatchNorm1d(21),\n",
    "          nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "    \n",
    "\n",
    "class Transformation(nn.Module):\n",
    "    \n",
    "    def __init__(self, rotation, translation):\n",
    "        super().__init__()\n",
    "        self.rotation = rotation\n",
    "        self.translation = translation\n",
    "        self.br = torch.transpose(rotation, 1, 0)\n",
    "        self.bt = torch.matmul(self.br, translation)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        reprojected_pt = torch.matmul(self.rotation,data) + self.translation#[:,None]\n",
    "        return reprojected_pt\n",
    "    \n",
    "    def backward(self, data):\n",
    "        reprojected_pt = torch.matmul(self.br,data) + self.bt#[:,None]\n",
    "        return reprojected_pt\n",
    "        \n",
    "class ExtendedSpatialSoftargMax(nn.Module):\n",
    "  \n",
    "    def __init__(self, height, width, channel, temperature=None, data_format='NCHW'):\n",
    "        super(ExtendedSpatialSoftargMax, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channel = channel\n",
    "\n",
    "        if temperature:\n",
    "            self.temperature = Parameter(torch.ones(1)*temperature)\n",
    "        else:\n",
    "            self.temperature = 1.\n",
    "\n",
    "        pos_x, pos_y = np.meshgrid(\n",
    "                np.linspace(-1., 1., self.height),\n",
    "                np.linspace(-1., 1., self.width)\n",
    "                )\n",
    "        pos_x = torch.from_numpy(pos_x.reshape(self.height*self.width)).float()\n",
    "        pos_y = torch.from_numpy(pos_y.reshape(self.height*self.width)).float()\n",
    "        self.register_buffer('pos_x', pos_x)\n",
    "        self.register_buffer('pos_y', pos_y)\n",
    "\n",
    "    def forward(self, feature, depth):\n",
    "        # Output:\n",
    "        #   (N, C*2) x_0 y_0 ...\n",
    "        if self.data_format == 'NHWC':\n",
    "            feature = feature.transpose(1, 3).tranpose(2, 3).view(-1, self.height*self.width)\n",
    "        else:\n",
    "            feature = feature.view(-1, self.height*self.width)\n",
    "\n",
    "        softmax_attention = F.softmax(feature/self.temperature, dim=-1)\n",
    "        expected_x = torch.sum(self.pos_x*softmax_attention, dim=1, keepdim=True)\n",
    "        expected_y = torch.sum(self.pos_y*softmax_attention, dim=1, keepdim=True)\n",
    "#         expected_xy = torch.cat([expected_x, expected_y], 1)\n",
    "#         feature_keypoints = expected_xy.view(-1, self.channel*2)\n",
    "        \n",
    "        image_height = depth.shape[3]\n",
    "        image_weight = depth.shape[2]\n",
    "        \n",
    "        ix = torch.round(expected_x * image_weight).long()\n",
    "        iy = torch.round(expected_y * image_height).long()\n",
    "        \n",
    "        z = depth[:, 0, ix, iy]\n",
    "        \n",
    "        result = torch.cat([expected_x * z, expected_y * z, z], 1)\n",
    "        feature_keypoints = result.view(-1, self.channel*3)\n",
    "\n",
    "        return feature_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04615bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyframe(nn.Module):\n",
    "    def __init__(self, rotation, translation):\n",
    "        super(Keyframe, self).__init__()\n",
    "        self.feature = FeatureNet()\n",
    "        self.extended_spatial_max = ExtendedSpatialSoftargMax(31,21,196)\n",
    "        self.transform = Transformation(rotation, translation)\n",
    "        self.pose_regress = PoseRegression()\n",
    "        self.rotation = rotation\n",
    "        self.translation = translation\n",
    "              \n",
    "    def forward(self, data, depth):\n",
    "        output = self.feature(data)\n",
    "        output = self.extended_spatial_max(output, depth)\n",
    "        output = self.transform(output)\n",
    "        output = self.pose_regress(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e43ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def euler2rotm(theta):\n",
    "    R_x = np.array([[1,         0,                  0                   ],\n",
    "                    [0,         math.cos(theta[0]), -math.sin(theta[0]) ],\n",
    "                    [0,         math.sin(theta[0]), math.cos(theta[0])  ]\n",
    "                    ])\n",
    "    R_y = np.array([[math.cos(theta[1]),    0,      math.sin(theta[1])  ],\n",
    "                    [0,                     1,      0                   ],\n",
    "                    [-math.sin(theta[1]),   0,      math.cos(theta[1])  ]\n",
    "                    ])         \n",
    "    R_z = np.array([[math.cos(theta[2]),    -math.sin(theta[2]),    0],\n",
    "                    [math.sin(theta[2]),    math.cos(theta[2]),     0],\n",
    "                    [0,                     0,                      1]\n",
    "                    ])            \n",
    "    R = np.dot(R_z, np.dot( R_y, R_x ))\n",
    "    return R\n",
    "\n",
    "def point_in_camera(point, rotation, translation):\n",
    "    reprojected_pt = torch.matmul(rotation,point) + translation#[:,None]\n",
    "    return reprojected_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2d98a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = torch.Tensor(euler2rotm([np.pi / 4, np.pi, -np.pi / 2]))\n",
    "translation = torch.Tensor(np.array([1.0, 0, 0.75]).reshape((3,1)))\n",
    "\n",
    "k = Keyframe(rotation, translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ffa6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The Yan Li, UTK, Knoxville, TN.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import torch\n",
    "import cv2 # for resize image\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "import random as r\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, target_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.mean = [0.485, 0.456, 0.406] \n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "        self.image_folder = join(data_dir, 'color')\n",
    "        self.depth_folder = join(data_dir, 'depth')\n",
    "        self.info_folder = join(data_dir, 'info')\n",
    "        self.action_folder = join(data_dir, 'action')\n",
    "        self.image_files = listdir(self.image_folder)\n",
    "        self.depth_files = listdir(self.depth_folder)\n",
    "        self.info_files = listdir(self.info_folder)\n",
    "        self.action_files = listdir(self.action_folder)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # image\n",
    "        data = self.loadfile(join(self.image_folder, self.image_files[idx]))\n",
    "        selected_frame = r.randint(1, data.shape[0])\n",
    "        img = data[selected_frame,0]\n",
    "        img = cv2.resize(img, dsize=(160, 120), interpolation=cv2.INTER_CUBIC)\n",
    "        img = img / 255.0\n",
    "        img = img - self.mean\n",
    "        img = img / self.std\n",
    "        \n",
    "        # depth\n",
    "        data = self.loadfile(join(self.depth_folder, self.depth_files[idx]))            \n",
    "        depth = data[selected_frame,0]\n",
    "        td = cv2.resize(depth, dsize=(160, 120), interpolation=cv2.INTER_CUBIC)        \n",
    "        depth = torch.Tensor(data[selected_frame,0])\n",
    "        color = torch.Tensor(img)\n",
    "        td = torch.Tensor(td)\n",
    "        td = torch.reshape(td, (120,160,1))\n",
    "        data = torch.cat((color, td), 2)        \n",
    "        data = data.permute(2, 0 ,1)\n",
    "        data = torch.reshape(data, (1, 4, 120, 160))\n",
    "        \n",
    "        # info\n",
    "        info = self.loadfile(join(self.info_folder, self.info_files[idx]))\n",
    "        #action \n",
    "        action = self.loadfile(join(self.action_folder, self.action_files[idx]))        \n",
    "        gt = self.resolveInfo(info, action, selected_frame)\n",
    "        \n",
    "        return data, depth, gt\n",
    "    \n",
    "    def loadfile(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def resolveInfo(self, info, action, selected_frame):\n",
    "\n",
    "        x = np.concatenate((np.array(info[selected_frame][5][0]), \n",
    "             np.array(info[selected_frame][5][1])),axis=0)\n",
    "        y = np.concatenate((np.array(info[selected_frame][6][0]), \n",
    "             np.array(info[selected_frame][6][1])), axis=0)\n",
    "        z = np.concatenate((np.array(action[selected_frame]['pose'][0]), \n",
    "             np.array(action[selected_frame]['pose'][1])),axis=0)\n",
    "        \n",
    "        gt = np.concatenate((x,y,z),axis=0)\n",
    "        gt = torch.Tensor(gt).reshape([1,gt.shape[0]])\n",
    "        return gt\n",
    "        \n",
    "#mean = [0.485, 0.456, 0.406] \n",
    "#std = [0.229, 0.224, 0.225]\n",
    "#\n",
    "#data_path = '../datasets/key_frame_identifier/block-insertion-test/'\n",
    "#img_folder = data_path + 'color/'\n",
    "#depth_folder = data_path + 'depth/'\n",
    "#pcl_file = '000000-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8a81fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'datasets/key_frame_identifier/block-insertion-test'\n",
    "data = dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e3afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "torch.Size([1, 21])\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "img, depth, gt = data[999]\n",
    "print(gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b6c07e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(7,1)\n",
    "y = torch.Tensor(7,1)\n",
    "z = torch.Tensor(7,1)\n",
    "\n",
    "o = torch.cat((x,y,z),dim=0)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e8b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import cv2 # for resize image\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "import random as r\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, target_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.mean = [0.485, 0.456, 0.406] \n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "        self.image_folder = join(data_dir, 'color')\n",
    "        self.depth_folder = join(data_dir, 'depth')\n",
    "        self.info_folder = join(data_dir, 'info')\n",
    "        self.action_folder = join(data_dir, 'action')\n",
    "        self.image_files = listdir(self.image_folder)\n",
    "        self.depth_files = listdir(self.depth_folder)\n",
    "        self.info_files = listdir(self.info_folder)\n",
    "        self.action_files = listdir(self.action_folder)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # image\n",
    "        data = self.loadfile(join(self.image_folder, self.image_files[idx]))\n",
    "        r.seed(datetime.datetime.now())\n",
    "        selected_frame = r.randint(1, data.shape[0])\n",
    "        img = data[selected_frame,0]\n",
    "        img = cv2.resize(img, dsize=(160, 120), interpolation=cv2.INTER_CUBIC)\n",
    "        img = img / 255.0\n",
    "        img = img - self.mean\n",
    "        img = img / self.std\n",
    "        \n",
    "        # depth\n",
    "        data = self.loadfile(join(self.depth_folder, self.depth_files[idx]))            \n",
    "        depth = data[selected_frame,0]\n",
    "        td = cv2.resize(depth, dsize=(160, 120), interpolation=cv2.INTER_CUBIC)\n",
    "        depth = torch.Tensor(depth)\n",
    "        color = torch.Tensor(img)\n",
    "        td = torch.Tensor(td)\n",
    "        td = torch.reshape(td, (120,160,1))\n",
    "        data = torch.cat((color, td), 2)        \n",
    "        data = data.permute(2, 0 ,1)\n",
    "        data = torch.reshape(data, (1, 4, 120, 160))\n",
    "        \n",
    "        # info\n",
    "        info = self.loadfile(join(self.info_folder, self.info_files[idx]))\n",
    "        #action \n",
    "        action = self.loadfile(join(self.action_folder, self.action_files[idx]))        \n",
    "        gt = self.resolveInfo(info, action, selected_frame)\n",
    "        \n",
    "        return data, depth, gt\n",
    "    \n",
    "    def loadfile(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def resolveInfo(self, info, action, selected_frame):\n",
    "        \n",
    "        print(selected_frame)\n",
    "\n",
    "        x = np.concatenate((np.array(info[selected_frame][5][0]), \n",
    "             np.array(info[selected_frame][5][1])),axis=0)\n",
    "        y = np.concatenate((np.array(info[selected_frame][6][0]), \n",
    "             np.array(info[selected_frame][6][1])), axis=0)\n",
    "        z = np.concatenate((np.array(action[selected_frame]['pose'][0]), \n",
    "             np.array(action[selected_frame]['pose'][1])),axis=0)\n",
    "        \n",
    "        gt = np.concatenate((x,y,z),axis=0)\n",
    "        gt = torch.Tensor(gt)\n",
    "#        gt = torch.Tensor(gt).reshape([1,gt.shape[0]])\n",
    "        return gt\n",
    "        \n",
    "#mean = [0.485, 0.456, 0.406] \n",
    "#std = [0.229, 0.224, 0.225]\n",
    "#\n",
    "#data_path = '../datasets/key_frame_identifier/block-insertion-test/'\n",
    "#img_folder = data_path + 'color/'\n",
    "#depth_folder = data_path + 'depth/'\n",
    "#pcl_file = '000000-1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3f3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "    \n",
    "class FeatureNet(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(4, 32, 3),\n",
    "          nn.Conv2d(32, 32, 3),\n",
    "          nn.Conv2d(32, 32, 3),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(32, 64, 3, stride=2),\n",
    "          nn.Conv2d(64, 128, 3),\n",
    "          nn.Conv2d(128, 128, 3),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(),    \n",
    "          nn.Conv2d(128, 256, 3, stride=2),\n",
    "          nn.Conv2d(256, 256, 3),\n",
    "          nn.Conv2d(256, 256, 3),\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.ReLU(),    \n",
    "          nn.Conv2d(256, 196, 1),\n",
    "          nn.BatchNorm2d(196),\n",
    "          nn.ReLU(),     \n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "    \n",
    "class PoseRegression(nn.Sequential):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Linear(588, 294),\n",
    "          nn.BatchNorm1d(294),\n",
    "          nn.Linear(294, 147),\n",
    "          nn.BatchNorm1d(147),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(147, 42),\n",
    "          nn.BatchNorm1d(42),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(42, 21),\n",
    "          nn.BatchNorm1d(21),\n",
    "          nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "\n",
    "\n",
    "class Transformation(nn.Module):\n",
    "    \n",
    "    def __init__(self, rotation, translation):\n",
    "        super().__init__()\n",
    "        self.rotation = rotation\n",
    "        self.translation = translation\n",
    "        self.br = torch.transpose(rotation, 1, 0)\n",
    "        self.bt = torch.matmul(self.br, translation)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        reprojected_pt = torch.matmul(self.rotation,data) + self.translation#[:,None]\n",
    "        return reprojected_pt\n",
    "    \n",
    "    def backward(self, data):\n",
    "        reprojected_pt = torch.matmul(self.br,data) + self.bt#[:,None]\n",
    "        return reprojected_pt\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13b728f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyframe(nn.Module):\n",
    "    def __init__(self, rotation, translation, spatial_height=31, \n",
    "                 spatial_weight=21, spatial_channel=196):\n",
    "        super(Keyframe, self).__init__()\n",
    "        self.feature = FeatureNet()\n",
    "#        self.extended_spatial_max = ExtendedSpatialSoftargMax(31,21,196)\n",
    "        self.extended_spatial_max = ExtendedSpatialSoftargMax(spatial_height, \n",
    "                                                              spatial_weight, \n",
    "                                                              spatial_channel)\n",
    "        self.transform = Transformation(rotation, translation)\n",
    "        self.pose_regress = PoseRegression()\n",
    "        self.rotation = rotation\n",
    "        self.translation = translation\n",
    "              \n",
    "    def forward(self, data, depth):\n",
    "        output = self.feature(data)\n",
    "        output = self.extended_spatial_max(output, depth)\n",
    "        output = self.transform(output)\n",
    "        output = self.pose_regress(output)\n",
    "        return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00b1ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "def selectDevice():        \n",
    "    torch.cuda.is_available()    \n",
    "    if torch.cuda.is_available():  \n",
    "      dev = \"cuda:0\" \n",
    "    else:  \n",
    "      dev = \"cpu\"      \n",
    "    return torch.device(dev)  \n",
    "\n",
    "\n",
    "def euler2rotm(theta):\n",
    "    R_x = np.array([[1,         0,                  0                   ],\n",
    "                    [0,         math.cos(theta[0]), -math.sin(theta[0]) ],\n",
    "                    [0,         math.sin(theta[0]), math.cos(theta[0])  ]\n",
    "                    ])\n",
    "    R_y = np.array([[math.cos(theta[1]),    0,      math.sin(theta[1])  ],\n",
    "                    [0,                     1,      0                   ],\n",
    "                    [-math.sin(theta[1]),   0,      math.cos(theta[1])  ]\n",
    "                    ])         \n",
    "    R_z = np.array([[math.cos(theta[2]),    -math.sin(theta[2]),    0],\n",
    "                    [math.sin(theta[2]),    math.cos(theta[2]),     0],\n",
    "                    [0,                     0,                      1]\n",
    "                    ])            \n",
    "    R = np.dot(R_z, np.dot( R_y, R_x ))\n",
    "    return R\n",
    "\n",
    "\n",
    "def camTrans():\n",
    "    rotation = torch.Tensor(euler2rotm([np.pi / 4, np.pi, -np.pi / 2]))\n",
    "    translation = torch.Tensor(np.array([1.0, 0, 0.75]).reshape((3,1)))\n",
    "    \n",
    "    return rotation, translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449c3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "\n",
    "class ExtendedSpatialSoftargMax(nn.Module):\n",
    "  \n",
    "    def __init__(self, height, width, channel, temperature=None, data_format='NCHW'):\n",
    "        super(ExtendedSpatialSoftargMax, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channel = channel\n",
    "\n",
    "        if temperature:\n",
    "            self.temperature = Parameter(torch.ones(1)*temperature)\n",
    "        else:\n",
    "            self.temperature = 1.\n",
    "\n",
    "        pos_x, pos_y = np.meshgrid(\n",
    "                np.linspace(-1., 1., self.height),\n",
    "                np.linspace(-1., 1., self.width)\n",
    "                )\n",
    "        pos_x = torch.from_numpy(pos_x.reshape(self.height*self.width)).float()\n",
    "        pos_y = torch.from_numpy(pos_y.reshape(self.height*self.width)).float()\n",
    "        self.register_buffer('pos_x', pos_x)\n",
    "        self.register_buffer('pos_y', pos_y)\n",
    "\n",
    "    def forward(self, feature, depth):\n",
    "        # Output:\n",
    "        #   (N, C*2) x_0 y_0 ...\n",
    "        if self.data_format == 'NHWC':\n",
    "            feature = feature.transpose(1, 3).tranpose(2, 3).view(-1, self.height*self.width)\n",
    "        else:\n",
    "            feature = feature.view(-1, self.height*self.width)\n",
    "\n",
    "        softmax_attention = F.softmax(feature/self.temperature, dim=-1)\n",
    "        expected_x = torch.sum(self.pos_x*softmax_attention, dim=1, keepdim=True)\n",
    "        expected_y = torch.sum(self.pos_y*softmax_attention, dim=1, keepdim=True)\n",
    "#         expected_xy = torch.cat([expected_x, expected_y], 1)\n",
    "#         feature_keypoints = expected_xy.view(-1, self.channel*2)\n",
    "        \n",
    "        image_height = depth.shape[3]\n",
    "        image_weight = depth.shape[2]\n",
    "        \n",
    "        ix = torch.round(expected_x * image_weight).long()\n",
    "        iy = torch.round(expected_y * image_height).long()\n",
    "        \n",
    "        z = depth[:, 0, ix, iy] \n",
    "        z_prime_x = z / 900 * image_weight\n",
    "        z_prime_y = z / 900 * image_height\n",
    "        \n",
    "        result = torch.cat([expected_x * z_prime_x, expected_y * z_prime_y, z], 1)\n",
    "        feature_keypoints = result.view(-1, self.channel*3)\n",
    "\n",
    "        return feature_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb145dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing dataset\n",
      "setting up loss function\n",
      "setup optimizer\n",
      "starting the training process\n",
      "74\n",
      "10\n",
      "44\n",
      "71\n",
      "64\n",
      "31\n",
      "68\n",
      "90\n",
      "56\n",
      "11\n",
      "47\n",
      "7\n",
      "19\n",
      "19\n",
      "41\n",
      "83\n",
      "24\n",
      "29\n",
      "29\n",
      "80\n",
      "103\n",
      "85\n",
      "82\n",
      "4\n",
      "10\n",
      "28\n",
      "72\n",
      "13\n",
      "92\n",
      "46\n",
      "45\n",
      "51\n",
      "34\n",
      "84\n",
      "43\n",
      "44\n",
      "85\n",
      "79\n",
      "35\n",
      "9\n",
      "102\n",
      "93\n",
      "43\n",
      "23\n",
      "86\n",
      "56\n",
      "23\n",
      "59\n",
      "87\n",
      "30\n",
      "91\n",
      "86\n",
      "34\n",
      "79\n",
      "33\n",
      "28\n",
      "111\n",
      "38\n",
      "3\n",
      "63\n",
      "17\n",
      "76\n",
      "88\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-41-ae80c77d169e>\", line 56, in __getitem__\n    gt = self.resolveInfo(info, action, selected_frame)\n  File \"<ipython-input-41-ae80c77d169e>\", line 72, in resolveInfo\n    z = np.concatenate((np.array(action[selected_frame]['pose'][0]),\nTypeError: 'NoneType' object is not subscriptable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-aeb7171c7666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-aeb7171c7666>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#        log_dir = os.path.join(data_path, 'logs', curr_time, 'train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-41-ae80c77d169e>\", line 56, in __getitem__\n    gt = self.resolveInfo(info, action, selected_frame)\n  File \"<ipython-input-41-ae80c77d169e>\", line 72, in resolveInfo\n    z = np.concatenate((np.array(action[selected_frame]['pose'][0]),\nTypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "47\n",
      "66\n",
      "63\n",
      "26\n",
      "97\n",
      "63\n",
      "71\n",
      "26\n",
      "34\n",
      "65\n",
      "80\n",
      "51\n",
      "66\n",
      "17\n",
      "9\n",
      "81\n",
      "72\n",
      "31\n",
      "10\n",
      "34\n",
      "13\n",
      "2\n",
      "71\n",
      "72\n",
      "1\n",
      "39\n",
      "7\n",
      "77\n",
      "30\n",
      "68\n",
      "86\n",
      "23\n",
      "46\n",
      "9\n",
      "37\n",
      "55\n",
      "73\n",
      "33\n",
      "27\n",
      "78\n",
      "84\n",
      "10\n",
      "42\n",
      "46\n",
      "48\n",
      "49\n",
      "33\n",
      "72\n",
      "81\n",
      "6\n",
      "76\n",
      "24\n",
      "53\n",
      "101\n",
      "65\n",
      "31\n",
      "39\n",
      "69\n",
      "99\n",
      "75\n",
      "57\n",
      "65\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The Yan Li, UTK, Knoxville, TN.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = selectDevice()\n",
    "    \n",
    "    rot, t = camTrans()\n",
    "    \n",
    "    model = Keyframe(rot, t).to(device)    \n",
    "    # preparing dataset\n",
    "    print('preparing dataset')\n",
    "    data_path = '/workspace/datasets/key_frame_identifier/block-insertion-test/'    \n",
    "    training_set = dataset(data_path)\n",
    "    params = {'batch_size': 64,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 1}    \n",
    "    train_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "    # setup loss fucntion\n",
    "    print('setting up loss function')\n",
    "    criterion = nn.MSELoss()\n",
    "    # setup optimizer \n",
    "    print('setup optimizer')\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # maximum epochs\n",
    "    max_epochs = 200000\n",
    "    \n",
    "    # start the training\n",
    "    print('starting the training process')\n",
    "    for epoch in range(max_epochs):\n",
    "#        name = f'{FLAGS.task}-{FLAGS.agent}-{FLAGS.n_demos}-{train_run}'\n",
    "        \n",
    "#        curr_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "#        log_dir = os.path.join(data_path, 'logs', curr_time, 'train')\n",
    "#        \n",
    "        for data, depth, gt in train_generator:\n",
    "            print(data.shape, depth.shape, gt.shape)\n",
    "            ps = model(data, depth)\n",
    "            loss = criterion(ps, gt)\n",
    "            if t % 100 == 99:\n",
    "                print(t, loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()            \n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#and the access its now method simpler\n",
    "d1 = datetime.now()\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7e4f6280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "torch.Size([64, 196])\n",
      "torch.Size([12544, 1])\n",
      "torch.Size([12544, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand([12544,1])\n",
    "y = torch.rand([12544,1])\n",
    "x[x>1] = 1\n",
    "x[x<-1] = -1\n",
    "y[y>1] = 1\n",
    "y[y<-1] = -1\n",
    "\n",
    "coord = (x * 320 + 320) + (y * 240 + 239) * 640\n",
    "\n",
    "print(sum(coord>480*640))\n",
    "\n",
    "baseline = torch.arange(0, 64) * 640*480.0\n",
    "baseline = baseline.repeat(196,1)\n",
    "# baseline = np.meshgrid(1,64) * 640*480\n",
    "# print(np.array(baseline).shape)\n",
    "baseline = torch.transpose(baseline, 1, 0)\n",
    "print(baseline.shape)\n",
    "baseline = baseline.reshape(64 * 196, 1)\n",
    "print(baseline.shape)\n",
    "print(coord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bd16fe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12544, 1])\n",
      "torch.Size([19660800])\n",
      "torch.Size([12544, 1])\n"
     ]
    }
   ],
   "source": [
    "coord = torch.round(coord + baseline).long()\n",
    "\n",
    "print(coord.shape)\n",
    "d = torch.rand([64*480*640])\n",
    "print(d.shape)\n",
    "z = torch.take(d, coord)\n",
    "print(z.shape)\n",
    "# print(baseline[1:12544:196])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dccb7fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186aabd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
